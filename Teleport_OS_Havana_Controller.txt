# Install OpenStack Grizzly on Ubuntu 12.04 LTS
#
# Controller setup steps on a Cloud based VM with:
# 4GB RAM, 2CPU
# eth0 - Public Interface [192.237.168.211]
# eth1 - Private Interface [10.208.143.46]
# /dev/xvda [150GB]
# /dev/xvdb [100GB] (Cinder Logical Volume]
#
# bugtrack al.kari@detacloud.com || @detacloud
# 2013-10-10

### Replace the following credentials for production install:
# mysqlpw
# keystonepw
# neutronpw
# novapw
# cinderpw
# glancepw
# ADMINSERVICETOKEN
# LabTenant
# labuser / labuserpw
# labadmin / labadminpw
###

################################################################
# PRE-PREP
################################################################

# Check if nested virtualization is supported. Use qemu if not.
apt-get install -y cpu-checker
kvm-ok

egrep -c '(vmx|svm)' /proc/cpuinfo

# use qemu if 0, else use kvm
# nova-compute-kvm requires that your CPU supports hardware-assisted virtualization (HVM) such as Intel VT-x or AMD-V. If your CPU does not support this, or if you are already running in a virtualized environment, you can instead use the nova-compute-qemu package. This package provides software-based virtualization.

# Add a user
adduser labuser
# Enter: labuserpw
apt-get install sudo -y
echo "labuser ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers

# Store IP Address information on reboot. 
vi /etc/provile

# Add at the bottom of /etc/profile
if [ -n "$PS1" ]; then
	export PUBLIC_IP="$(ifconfig | grep -A 1 'eth0' | tail -1 | cut -d ':' -f 2 | cut -d ' ' -f 1)"
	export PRIVATE_IP="$(ifconfig | grep -A 1 'eth1' | tail -1 | cut -d ':' -f 2 | cut -d ' ' -f 1)"
fi


################################################################
# User Prep Host
################################################################


# 3 - Install supporting packages
sudo apt-get -y install curl ntp

# 4 - Synchronize time
sudo ntpdate -u 129.6.15.30 


# Disable IP source route verification
sudo sed -i "s/#net.ipv4.conf.default.rp_filter=1/net.ipv4.conf.default.rp_filter=0/g" /etc/sysctl.conf
sudo sed -i "s/#net.ipv4.conf.all.rp_filter=1/net.ipv4.conf.all.rp_filter=0/g" /etc/sysctl.conf
sudo sysctl -p
sudo service networking restart

# Set hostname
sudo hostname controller
sudo -- sh -c "echo controller > /etc/hostname"

sudo -- sh -c "echo $PUBLIC_IP controller > /etc/hosts"
sudo -- sh -c "echo $PRIVATE_IP controller >> /etc/hosts"

# IMPORTANT: Reboot after changing host name
sudo shutdown -r now

# LOGIN WITH NEW USER: labuser/labuserpw


# 5- Enable the Cloud Archive repository
sudo -- sh -c " cat >> /etc/apt/sources.list.d/grizzly.list <<EOF
deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/havana main
EOF"

sudo apt-get install ubuntu-cloud-keyring

# Install pgp key
gpg --keyserver keyserver.ubuntu.com --recv-keys 5EDB1B62EC4926EA
sudo -- sh -c "gpg --armor --export EC4926EA | apt-key add -"


# 6 - Update all
sudo apt-get update -y && sudo apt-get upgrade -y


####################################################################################
####################################################################################
#				MySQL and RabbitMQ
####################################################################################
####################################################################################

# 7 - Preseed the MySQL install
cat <<EOF | sudo debconf-set-selections
mysql-server-5.1 mysql-server/root_password password mysqlpw
mysql-server-5.1 mysql-server/root_password_again password mysqlpw
mysql-server-5.1 mysql-server/start_on_boot boolean true
EOF

# 8 - Install packages and dependencies
sudo apt-get install -y rabbitmq-server mysql-server python-mysqldb

# 9- Configure MySQL to listen on all interfaces
sudo sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf
sudo service mysql restart


####################################################################################
####################################################################################
#				Identity Service (Keystone)
####################################################################################
####################################################################################

# Install Keystone
sudo apt-get install -y keystone

# Delete the keystone.db file created in the /var/lib/keystone directory.
sudo rm /var/lib/keystone/keystone.db

# Verify the OpenStack version (Havana == 2013.2)
sudo dpkg -p keystone | grep "Version:"

# Create a database for keystone
sudo mysql -u root -pmysqlpw -e "CREATE DATABASE keystone;"
sudo mysql -u root -pmysqlpw -e "GRANT ALL ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystonepw';"
sudo mysql -u root -pmysqlpw -e "GRANT ALL ON keystone.* TO 'keystone'@'$PRIVATE_IP' IDENTIFIED BY 'keystonepw';"
sudo mysql -u root -pmysqlpw -e "GRANT ALL ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystonepw';"


# Configure keystone to use MySQL
sudo sed -i "s|connection = sqlite:////var/lib/keystone/keystone.db|connection = mysql://keystone:keystonepw@$PRIVATE_IP/keystone|g" /etc/keystone/keystone.conf

# Change default admin_token
sudo sed -i "s|# admin_token = ADMIN|admin_token = ADMINSERVICETOKEN|g" /etc/keystone/keystone.conf

# Create encryption certificate to use ssl with other services
########################################################## sudo keystone-manage pki_setup

########################################################## sudo -- sh -c "sudo chown -R keystone:keystone /etc/keystone/*"


# Restart keystone service
sudo service keystone restart

# Verify keystone service successfully restarted
sudo pgrep -l keystone

# Initialize the database schema
sudo -u keystone keystone-manage db_sync

# Export the keystone "admin" credentials to add the first users
export SERVICE_TOKEN=ADMINSERVICETOKEN
export SERVICE_ENDPOINT=http://$PRIVATE_IP:35357/v2.0

# Create new tenants (The services tenant will be used later when configuring services to use keystone)
TENANT_ID=`keystone tenant-create --name LabTenant | awk '/ id / { print $4 }'`
SERVICE_TENANT_ID=`keystone tenant-create --name Services | awk '/ id / { print $4 }'`
# 22- Verify
keystone tenant-list

# Create admin role
ADMIN_ROLE_ID=`keystone role-create --name admin | awk '/ id / { print $4 }'`

# Create new users
MEMBER_USER_ID=`keystone user-create --tenant-id $TENANT_ID --name labuser --pass labuserpw | awk '/ id / { print $4 }'`
ADMIN_USER_ID=`keystone user-create --tenant-id $TENANT_ID --name labadmin --pass labadminpw | awk '/ id / { print $4 }'`

# Grant admin role
keystone user-role-add --user-id $ADMIN_USER_ID --tenant-id $TENANT_ID --role-id $ADMIN_ROLE_ID
# 25 - Verify
keystone user-list

# List the new tenant, users, roles, and role assigments
keystone tenant-list
keystone role-list
keystone user-list --tenant-id $TENANT_ID
keystone user-role-list --tenant-id $TENANT_ID --user-id $MEMBER_USER_ID
keystone user-role-list --tenant-id $TENANT_ID --user-id $ADMIN_USER_ID

# Populate the services in the service catalog
KEYSTONE_SVC_ID=`keystone service-create --name=keystone --type=identity --description="Keystone Identity Service" | awk '/ id / { print $4 }'`
GLANCE_SVC_ID=`keystone service-create --name=glance --type=image --description="Glance Image Service" | awk '/ id / { print $4 }'`
NEUTRON_SVC_ID=`keystone service-create --name=neutron --type=network --description="Neutron Network Service" | awk '/ id / { print $4 }'`
NOVA_SVC_ID=`keystone service-create --name=nova --type=compute --description="Nova Compute Service" | awk '/ id / { print $4 }'`
CINDER_SVC_ID=`keystone service-create --name=cinder --type=volume --description="Cinder Volume Service" | awk '/ id / { print $4 }'`

# List the new services
keystone service-list

# Populate the endpoints in the service catalog
keystone endpoint-create --region RegionOne --service-id=$KEYSTONE_SVC_ID --publicurl=http://$PRIVATE_IP:5000/v2.0 --internalurl=http://$PRIVATE_IP:5000/v2.0 --adminurl=http://$PRIVATE_IP:35357/v2.0
keystone endpoint-create --region RegionOne --service-id=$GLANCE_SVC_ID --publicurl=http://$PRIVATE_IP:9292/v1 --internalurl=http://$PRIVATE_IP:9292/v1 --adminurl=http://$PRIVATE_IP:9292/v1
keystone endpoint-create --region RegionOne --service-id=$NEUTRON_SVC_ID --publicurl=http://$PRIVATE_IP:9696/ --internalurl=http://$PRIVATE_IP:9696/ --adminurl=http://$PRIVATE_IP:9696/
keystone endpoint-create --region RegionOne --service-id=$NOVA_SVC_ID --publicurl="http://$PRIVATE_IP:8774/v2/%(tenant_id)s" --internalurl="http://$PRIVATE_IP:8774/v2/%(tenant_id)s" --adminurl="http://$PRIVATE_IP:8774/v2/%(tenant_id)s"
keystone endpoint-create --region RegionOne --service-id=$CINDER_SVC_ID --publicurl="http://$PRIVATE_IP:8776/v1/%(tenant_id)s" --internalurl="http://$PRIVATE_IP:8776/v1/%(tenant_id)s" --adminurl="http://$PRIVATE_IP:8776/v1/%(tenant_id)s"

# List the new endpoints
keystone endpoint-list

# The keystone "admin" credentials are no longer needed
unset SERVICE_TOKEN
unset SERVICE_ENDPOINT

# Verify identity service is functioning
curl -d '{"auth": {"tenantName": "LabTenant", "passwordCredentials": {"username": "labuser", "password": "labuserpw"}}}' -H "Content-type: application/json" http://$PRIVATE_IP:5000/v2.0/tokens | python -m json.tool
curl -d '{"auth": {"tenantName": "LabTenant", "passwordCredentials": {"username": "labadmin", "password": "labadminpw"}}}' -H "Content-type: application/json" http://$PRIVATE_IP:5000/v2.0/tokens | python -m json.tool

# Create the 'user' and 'admin' credentials
mkdir ~/credentials

cat >> ~/credentials/user <<EOF
export OS_USERNAME=labuser
export OS_PASSWORD=labuserpw
export OS_TENANT_NAME=LabTenant
export OS_AUTH_URL=http://$PRIVATE_IP:5000/v2.0/
export OS_REGION_NAME=RegionOne
export OS_NO_CACHE=1
EOF

cat >> ~/credentials/admin <<EOF
export OS_USERNAME=labadmin
export OS_PASSWORD=labadminpw
export OS_TENANT_NAME=LabTenant
export OS_AUTH_URL=http://$PRIVATE_IP:5000/v2.0/
export OS_REGION_NAME=RegionOne
export OS_NO_CACHE=1
EOF

# Use the 'admin' credentials
source ~/credentials/admin

cat >> ~/.bashrc <<EOF
export SERVICE_TENANT_ID=$SERVICE_TENANT_ID
export ADMIN_ROLE_ID=$ADMIN_ROLE_ID
EOF


####################################################################################
####################################################################################
#				Image Servie (Glance)
####################################################################################
####################################################################################

# Install the image service (glance)
sudo apt-get install -y glance

sudo rm /var/lib/glance/glance.sqlite

# Use the 'admin' credentials to add the glance service user in keystone
# source ~/credentials/admin

# Create glance service user in the services tenant
GLANCE_USER_ID=`keystone user-create --tenant-id $SERVICE_TENANT_ID --name glance --pass glancepw | awk '/ id / { print $4 }'`

# Grant admin role to glance service user
keystone user-role-add --user-id $GLANCE_USER_ID --tenant-id $SERVICE_TENANT_ID --role-id $ADMIN_ROLE_ID

# List the new user and role assignment
keystone user-list --tenant-id $SERVICE_TENANT_ID
keystone user-role-list --tenant-id $SERVICE_TENANT_ID --user-id $GLANCE_USER_ID

# Create a database for glance
mysql -u root -pmysqlpw -e "CREATE DATABASE glance;"
mysql -u root -pmysqlpw -e "GRANT ALL ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glancepw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON glance.* TO 'glance'@'$PRIVATE_IP' IDENTIFIED BY 'glancepw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glancepw';"

# Configure the glance-api service
sudo sed -i "s|sql_connection = sqlite:////var/lib/glance/glance.sqlite|sql_connection = mysql://glance:glancepw@$PRIVATE_IP/glance|g" /etc/glance/glance-api.conf
sudo sed -i "s/auth_host = 127.0.0.1/auth_host = $PRIVATE_IP/g" /etc/glance/glance-api.conf
sudo sed -i 's/%SERVICE_TENANT_NAME%/Services/g' /etc/glance/glance-api.conf
sudo sed -i 's/%SERVICE_USER%/glance/g' /etc/glance/glance-api.conf
sudo sed -i 's/%SERVICE_PASSWORD%/glancepw/g' /etc/glance/glance-api.conf
sudo sed -i 's/#flavor=/flavor = keystone/g' /etc/glance/glance-api.conf

# Configure the glance-registry service
sudo sed -i "s|sql_connection = sqlite:////var/lib/glance/glance.sqlite|sql_connection = mysql://glance:glancepw@$PRIVATE_IP/glance|g" /etc/glance/glance-registry.conf
sudo sed -i "s/auth_host = 127.0.0.1/auth_host = $PRIVATE_IP/g" /etc/glance/glance-registry.conf
sudo sed -i 's/%SERVICE_TENANT_NAME%/Services/g' /etc/glance/glance-registry.conf
sudo sed -i 's/%SERVICE_USER%/glance/g' /etc/glance/glance-registry.conf
sudo sed -i 's/%SERVICE_PASSWORD%/glancepw/g' /etc/glance/glance-registry.conf
sudo sed -i 's/#flavor=/flavor = keystone/g' /etc/glance/glance-registry.conf

# Restart glance services
sudo service glance-registry restart
sudo service glance-api restart

# Verify glance services successfully restarted
pgrep -l glance

# Initialize the database schema. A deprecation warning is OK...
sudo -u glance glance-manage db_sync

# Download some images
mkdir ~/images
wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-uec.tar.gz -qO- | tar zxC ~/images
wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img -O ~/images/cirros-0.3.1~pre4-x86_64-disk.img
#Optional Ubuntu Image
wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img -O ~/images/precise.qcow2.img
#Optional Windows Image
wget http://192.237.168.211/images/windows_server_2012_standard_eval_kvm_20130510.qcow2.gz  -O ~/images/windows_server_2012_standard_eval_kvm_20130510.qcow2.gz

# Use the 'user' credentials
source ~/credentials/user

# Register a three part image (amazon style, separate kernal and ramdisk)
KERNEL_ID=`glance image-create --name "cirros-threepart-kernel" --disk-format aki --container-format aki --is-public True --file ~/images/cirros-0.3.0-x86_64-vmlinuz | awk '/ id / { print $4 }'`
RAMDISK_ID=`glance image-create --name "cirros-threepart-ramdisk" --disk-format ari --container-format ari --is-public True --file ~/images/cirros-0.3.0-x86_64-initrd | awk '/ id / { print $4 }'`
IMAGE_ID_1=`glance image-create --name "cirros-threepart" --disk-format ami --container-format ami --is-public True --property kernel_id=$KERNEL_ID --property ramdisk_id=$RAMDISK_ID --file ~/images/cirros-0.3.0-x86_64-blank.img | awk '/ id / { print $4 }'`

# Register a qcow2 image
IMAGE_ID_2=`glance image-create --name "cirros-qcow2" --disk-format qcow2 --container-format bare --is-public True --file ~/images/cirros-0.3.1~pre4-x86_64-disk.img | awk '/ id / { print $4 }'`

# Optional Ubuntu
UBUNTU_ID=`glance image-create --name "precise-qcow2" --disk-format qcow2 --container-format bare --is-public True --file ~/images/precise.qcow2.img | awk '/ id / { print $4 }'`
# Optional Windows
WIN_IMAGE_ID=`gunzip -cd windows_server_2012_standard_eval_kvm_20130510.qcow2.gz | glance image-create --name "Windwos Server 2012 Std eval" --disk-format qcow2 --container-format bare --is-public True | awk '/ id / { print $4 }'`

# Verify the images exist in glance
glance image-list

# Examine details of images
glance image-show $IMAGE_ID_1
glance image-show $IMAGE_ID_2
# Optional
glance image-show $UBUNTU_ID
glance image-show $WINIMAGE_ID


cat >> ~/.bashrc <<EOF
export IMAGE_ID_1=$IMAGE_ID_1
export IMAGE_ID_2=$IMAGE_ID_2
export UBUNTU_ID=$UBUNTU_ID
export WINIMAGE_ID=$WINIMAGE_ID
EOF


####################################################################################
####################################################################################
#				Networking service (Neutron)
####################################################################################
####################################################################################

###### VALIDATE AGAINST http://docs.openstack.org/network-admin/admin/content/demo_routers_with_private_networks_installions.html

# Install dependencies
sudo apt-get install -y openvswitch-switch

# Install the network service (neutron)
sudo apt-get install -y neutron-server

# Neutron includes plugins, next line is not required
# sudo apt-get install -y neutron-plugin-openvswitch

# Install the network service (neutron) agents
sudo apt-get install -y neutron-plugin-openvswitch-agent neutron-dhcp-agent neutron-l3-agent

# Create a database for neutron
mysql -u root -pmysqlpw -e "CREATE DATABASE neutron;"
mysql -u root -pmysqlpw -e "GRANT ALL ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutronpw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON neutron.* TO 'neutron'@'$PRIVATE_IP' IDENTIFIED BY 'neutronpw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutronpw';"

# Configure the neutron OVS plugin
# VALIDATE AGINST http://docs.openstack.org/network-admin/admin/content/ovs_neutron_plugin.html

sudo sed -i "s|# connection = mysql://root:nova@127.0.0.1:3306/ovs_neutron|sql_connection = mysql://neutron:neutronpw@$PRIVATE_IP/neutron?charset=utf8|g"  /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
sudo sed -i 's/# enable_tunneling = False/enable_tunneling = True/g' /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
sudo sed -i 's/# Example: tenant_network_type = gre/tenant_network_type = gre/g' /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
sudo sed -i 's/# tunnel_id_ranges = 1:1000/tunnel_id_ranges = 1:1000/g' /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
sudo sed -i "s/# local_ip = 10.0.0.3/local_ip = $PUBLIC_IP/g" /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
sudo sed -i "s/# firewall_driver/firewall_driver/g" /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini
# sudo sed -i "s/# Example: tunnel_type = gre/tunnel_type = gre/g" /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini


# Use the 'admin' credentials to add the neutron service user in keystone
source ~/credentials/admin

# Create neutron service user in the services tenant
NEUTRON_USER_ID=`keystone user-create --tenant-id $SERVICE_TENANT_ID --name neutron --pass neutronpw | awk '/ id / { print $4 }'`

# Grant admin role to neutron service user
keystone user-role-add --user-id $NEUTRON_USER_ID --tenant-id $SERVICE_TENANT_ID --role-id $ADMIN_ROLE_ID

# List the new user and role assigment
keystone user-list --tenant-id $SERVICE_TENANT_ID
keystone user-role-list --tenant-id $SERVICE_TENANT_ID --user-id $NEUTRON_USER_ID

# Configure the neutron service to use keystone
######### VALIDATE AGAINST http://docs.openstack.org/network-admin/admin/content/neutron_conf.html

sudo sed -i 's/# auth_strategy = keystone/auth_strategy = keystone/g' /etc/neutron/neutron.conf
sudo sed -i "s/auth_host = 127.0.0.1/auth_host = $PRIVATE_IP/g" /etc/neutron/neutron.conf
sudo sed -i 's/%SERVICE_TENANT_NAME%/Services/g' /etc/neutron/neutron.conf
sudo sed -i 's/%SERVICE_USER%/neutron/g' /etc/neutron/neutron.conf
sudo sed -i 's/%SERVICE_PASSWORD%/neutronpw/g' /etc/neutron/neutron.conf
sudo sed -i 's/# control_exchange = neutron/control_exchange = neutron/g' /etc/neutron/neutron.conf
sudo sed -i "s/# rabbit_hosts = localhost:5672/rabbit_hosts = $PRIVATE_IP:5672/g" /etc/neutron/neutron.conf
sudo sed -i "s|connection = sqlite:////var/lib/neutron/neutron.sqlite|connection = mysql://neutron:neutronpw@$PRIVATE_IP/neutron?charset=utf8|g" /etc/neutron/neutron.conf

# ??? ## sudo sed -i 's/notifier.rpc_notifier/notifier.rabbit_notifier/g' /etc/neutron/neutron.conf

# ??? notification_driver = neutron.openstack.common.notifier.rpc_notifier -> notification_driver = neutron.openstack.common.notifier.rabbit_notifier



# Start Open vSwitch
sudo service openvswitch-switch restart

# Create the integration and external bridges
sudo ovs-vsctl add-br br-int
sudo ovs-vsctl add-br br-ex

# Restart the neutron services
sudo service neutron-server restart
sudo service neutron-plugin-openvswitch-agent restart
sudo service neutron-dhcp-agent restart
sudo service neutron-l3-agent restart

# Use the 'user' credentials
source ~/credentials/user

# Create a network (L2)
PRIVATE_NET_ID=`neutron net-create private | awk '/ id / { print $4 }'`

# List Networks
neutron net-list

# Create Subnet (L3)
PRIVATE_SUBNET1_ID=`neutron subnet-create --name private-subnet1 $PRIVATE_NET_ID 10.208.128.0/29 | awk '/ id / { print $4 }'`

# List Subnets
neutron subnet-list

# Examine details of network and subnet
neutron net-show $PRIVATE_NET_ID
neutron subnet-show $PRIVATE_SUBNET1_ID

cat >> ~/.bashrc <<EOF
export PRIVATE_NET_ID=$PRIVATE_NET_ID
export PRIVATE_SUBNET1_ID=$PRIVATE_SUBNET1_ID
EOF



# Other Neutron commands

# List system extensions
neutron ext-list -c alias -c name

# Create a port with specific IP address
# neutron port-create net1 --fixed-ip ip_address=192.168.2.40

neutron port-list



####################################################################################
####################################################################################
#				Compute Service (Nova)
####################################################################################
####################################################################################

# nova-compute-kvm requires that your CPU supports hardware-assisted virtualization (HVM) such as Intel VT-x or AMD-V. If your CPU does not support this, or if you are already running in a virtualized environment, you can instead use the nova-compute-qemu package. This package provides software-based virtualization.


# Install the compute service (nova) [S L O W]
#sudo apt-get install -y nova-api nova-scheduler nova-conductor nova-compute nova-cert nova-consoleauth genisoimage
sudo apt-get install -y nova-api nova-scheduler nova-conductor nova-compute-qemu nova-cert nova-consoleauth genisoimage

# Use the 'admin' credentials to add the nova service user in keystone
source ~/credentials/admin

# Create nova service user in the services tenant
NOVA_USER_ID=`keystone user-create --tenant-id $SERVICE_TENANT_ID --name nova --pass novapw | awk '/ id / { print $4 }'`

# Grant admin role to nova service user
keystone user-role-add --user-id $NOVA_USER_ID --tenant-id $SERVICE_TENANT_ID --role-id $ADMIN_ROLE_ID

# List the new user and role assigment
keystone user-list --tenant-id $SERVICE_TENANT_ID
keystone user-role-list --tenant-id $SERVICE_TENANT_ID --user-id $NOVA_USER_ID

# Create a database for nova
mysql -u root -pmysqlpw -e "CREATE DATABASE nova;"
mysql -u root -pmysqlpw -e "GRANT ALL ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'novapw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON nova.* TO 'nova'@'$PRIVATE_IP' IDENTIFIED BY 'novapw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON nova.* TO 'nova'@'%' IDENTIFIED BY 'novapw';"

# Configure nova
cat <<EOF | sudo tee -a /etc/nova/nova.conf
compute_driver=libvirt.LibvirtDriver
libvirt_type=qemu
libvirt_use_virtio_for_bridges=True
network_api_class=nova.network.neutronv2.api.API
neutron_url=http://$PRIVATE_IP:9696
neutron_auth_strategy=keystone
neutron_admin_tenant_name=Services
neutron_admin_username=neutron
neutron_admin_password=neutronpw
neutron_admin_auth_url=http://$PRIVATE_IP:35357/v2.0
firewall_driver=nova.virt.firewall.NoopFirewallDriver
security_group_api=neutron
sql_connection=mysql://nova:novapw@$PRIVATE_IP/nova
auth_strategy=keystone
force_config_drive=True
my_ip=$PRIVATE_IP
EOF

# Disable verbose logging
sudo sed -i 's/verbose=True/verbose=False/g' /etc/nova/nova.conf

# Configure nova to use keystone
sudo sed -i "s/auth_host = 127.0.0.1/auth_host = $PRIVATE_IP/g" /etc/nova/api-paste.ini
sudo sed -i 's/%SERVICE_TENANT_NAME%/Services/g' /etc/nova/api-paste.ini
sudo sed -i 's/%SERVICE_USER%/nova/g' /etc/nova/api-paste.ini
sudo sed -i 's/%SERVICE_PASSWORD%/novapw/g' /etc/nova/api-paste.ini

# Initialize the nova database
sudo -u nova nova-manage db sync

# Restart nova services
sudo service nova-api restart
sudo service nova-scheduler restart
sudo service nova-conductor restart
sudo service nova-compute restart
sudo service nova-cert restart
sudo service nova-consoleauth restart

# Restart the neutron services
sudo service neutron-server restart
sudo service neutron-plugin-openvswitch-agent restart
sudo service neutron-dhcp-agent restart
sudo service neutron-l3-agent restart

# Verify nova services successfully restarted
pgrep -l nova

# Verify nova services are functioning and checking in :-)
sudo nova-manage service list

# Use the 'user' credentials
source ~/credentials/user

# List images
nova image-list

# List flavors
nova flavor-list

# Add flavors
nova flavor-create m1.micro 6 256 0 1
nova flavor-create m1.nano 7 1024 0 1

# Boot an instance using flavor and image names (if names are unique)
nova boot --image cirros-qcow2 --flavor m1.tiny LabFirstInstance

# Boot an instance using flavor and image IDs
nova boot --image $IMAGE_ID_2 --flavor 1 LabSecondInstance

# List instances, notice status of instance
nova list

# Show details of instance
nova show LabFirstInstance

# View console log of instance
nova console-log LabFirstInstance

# Get network namespace (ie, qdhcp-5ab46e23-118a-4cad-9ca8-51d56a5b6b8c)
sudo ip netns
NETNS_ID=qdhcp-$PRIVATE_NET_ID

# Ping first instance after status is active
sudo ip netns exec $NETNS_ID ping -c 3 10.208.128.3

# Log into first instance ( username is 'cirros', password is 'cubswin:)' )
sudo ip netns exec $NETNS_ID ssh cirros@10.208.128.3

# If you get a 'REMOTE HOST IDENTIFICATION HAS CHANGED' warning from previous command
sudo ssh-keygen -f "/root/.ssh/known_hosts" -R 10.208.128.3

# Ping second instance from first instance
ping -c 3 10.208.128.4

# Log into second instance from first instance ( username is 'cirros', password is 'cubswin:)' )
ssh cirros@10.208.128.4

# Log out of second instance
exit

# Log out of first instance
exit

# Use virsh to talk directly to libvirt
sudo virsh list --all

# Delete instances
nova delete LabFirstInstance
nova delete LabSecondInstance

# List instances, notice status of instance
nova list

cat >> ~/.bashrc <<EOF
export NETNS_ID=$NETNS_ID
EOF


####################################################################################
####################################################################################
#				Volume Service (Cinder)
####################################################################################
####################################################################################

# Install the volume service (cinder)
sudo apt-get install -y cinder-api cinder-scheduler cinder-volume

# Use the 'admin' credentials to add the cinder service user in keystone
source ~/credentials/admin

# Create cinder service user in the services tenant
CINDER_USER_ID=`keystone user-create --tenant-id $SERVICE_TENANT_ID --name cinder --pass cinderpw | awk '/ id / { print $4 }'`

# Grant admin role to cinder service user
keystone user-role-add --user-id $CINDER_USER_ID --tenant-id $SERVICE_TENANT_ID --role-id $ADMIN_ROLE_ID

# List the new user and role assigment
keystone user-list --tenant-id $SERVICE_TENANT_ID
keystone user-role-list --tenant-id $SERVICE_TENANT_ID --user-id $CINDER_USER_ID

# Create a database for cinder
mysql -u root -pmysqlpw -e "CREATE DATABASE cinder;"
mysql -u root -pmysqlpw -e "GRANT ALL ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinderpw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON cinder.* TO 'cinder'@'$PRIVATE_IP' IDENTIFIED BY 'cinderpw';"
mysql -u root -pmysqlpw -e "GRANT ALL ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinderpw';"

# Configure cinder
( cat | sudo tee -a /etc/cinder/cinder.conf ) <<EOF
sql_connection = mysql://cinder:cinderpw@$PRIVATE_IP/cinder
my_ip = $PRIVATE_IP
EOF

# Configure cinder-api to use keystone
sudo sed -i "s/service_host = 127.0.0.1/service_host = $PRIVATE_IP/g" /etc/cinder/api-paste.ini
sudo sed -i "s/auth_host = 127.0.0.1/auth_host = $PRIVATE_IP/g" /etc/cinder/api-paste.ini
sudo sed -i 's/admin_tenant_name = %SERVICE_TENANT_NAME%/admin_tenant_name = Services/g' /etc/cinder/api-paste.ini
sudo sed -i 's/admin_user = %SERVICE_USER%/admin_user = cinder/g' /etc/cinder/api-paste.ini
sudo sed -i 's/admin_password = %SERVICE_PASSWORD%/admin_password = cinderpw/g' /etc/cinder/api-paste.ini

# Initialize the database schema
sudo -u cinder cinder-manage db sync

# Configure nova to use cinder
( cat | sudo tee -a /etc/nova/nova.conf ) <<EOF
volume_manager=cinder.volume.manager.VolumeManager
volume_api_class=nova.volume.cinder.API
enabled_apis=osapi_compute
EOF

# Restart nova-api to disable the nova-volume api (osapi_volume)
sudo service nova-api restart
sudo service nova-scheduler restart
sudo service nova-conductor restart
sudo service nova-compute restart
sudo service nova-cert restart
sudo service nova-consoleauth restart

# Restart the neutron services
sudo service neutron-server restart
sudo service neutron-plugin-openvswitch-agent restart
sudo service neutron-dhcp-agent restart
sudo service neutron-l3-agent restart

# Configure tgt
( cat | sudo tee -a /etc/tgt/targets.conf ) <<EOF
default-driver iscsi
EOF

# Restart tgt and open-iscsi
sudo service tgt restart
sudo service open-iscsi restart

# Create the volume group
sudo pvcreate /dev/xvda2
sudo vgcreate cinder-volumes /dev/xvda2

# Verify the volume group
sudo vgdisplay

# Restart the volume services
sudo service cinder-volume restart
sudo service cinder-scheduler restart
sudo service cinder-api restart

# Use the 'user' credentials
source ~/credentials/user

# Create a new volume
cinder create 1 --display-name LabFirstVolume

# Boot an instance to attach volume to
nova boot --image cirros-qcow2 --flavor m1.tiny LabVolumeInstance

# List instances, notice status of instance
nova list

# List volumes, notice status of volume
cinder list

# Attach volume to instance after instance is active, and volume is available
nova volume-attach <instance-id> <volume-id> /dev/vdc

# Log into first instance ( username is 'cirros', password is 'cubswin:)' )
sudo ip netns exec $NETNS_ID ssh cirros@10.208.128.3

# If you get a 'REMOTE HOST IDENTIFICATION HAS CHANGED' warning from previous command
sudo ssh-keygen -f "/root/.ssh/known_hosts" -R 10.208.128.3

# Make filesystem on volume
sudo mkfs.ext3 /dev/vdc

# Create a mountpoint
sudo mkdir /extraspace

# Mount the volume at the mountpoint
sudo mount /dev/vdc /extraspace

# Create a file on the volume
sudo touch /extraspace/helloworld.txt
sudo ls /extraspace

# Unmount the volume
sudo umount /extraspace

# Log out of instance
exit

# Detach volume from instance
nova volume-detach <instance-id> <volume-id>

# List volumes, notice status of volume
cinder list

# Delete instance
nova delete LabVolumeInstance

# From the dashboard, try booting another instance, attach volume to new instance, and mount volume. Data should be preserved on volume.


####################################################################################
####################################################################################
#				Dashboard (Horizon)
####################################################################################
####################################################################################


# Install dependencies
sudo apt-get install -y memcached novnc

# Install the dashboard (horizon)
sudo apt-get install -y openstack-dashboard nova-novncproxy

# Remove the Ubuntu theme
sudo apt-get remove -y --purge openstack-dashboard-ubuntu-theme

# Configure nova for VNC
( cat | sudo tee -a /etc/nova/nova.conf ) <<EOF
novncproxy_base_url=http://$PUBLIC_IP:6080/vnc_auto.html
vncserver_proxyclient_address=$PRIVATE_IP
vncserver_listen=0.0.0.0
novnc_enable=true
EOF

# Set default role
sudo sed -i 's/OPENSTACK_KEYSTONE_DEFAULT_ROLE = "Member"/OPENSTACK_KEYSTONE_DEFAULT_ROLE = "_member_"/g' /etc/openstack-dashboard/local_settings.py

# Restart the nova services
sudo service nova-api restart
sudo service nova-scheduler restart
sudo service nova-conductor restart
sudo service nova-compute restart
sudo service nova-cert restart
sudo service nova-consoleauth restart
sudo service apache2 restart

# Point your browser to http://<IP_ADDRESS>/horizon

####################################################################################
####################################################################################
#				Keypairs
####################################################################################
####################################################################################


# Create a nova keypair, saving private key
nova keypair-add LabKeypair > LabKeypair.pem

# Change private key permissions
chmod 600 LabKeypair.pem

# Boot an instance using the keypair
nova boot --image cirros-qcow2 --flavor m1.tiny --key-name LabKeypair LabKeypairInstance

# List instances, notice status of instance
nova list

# Ping instance after status is active
sudo ip netns exec $NETNS_ID ping -c 3 10.208.128.3

# Log into first instance ( you should be logged right into a shell, no password necessary )
sudo ip netns exec $NETNS_ID ssh -i LabKeypair.pem cirros@10.208.128.3

# If you get a 'REMOTE HOST IDENTIFICATION HAS CHANGED' warning from previous command
sudo ssh-keygen -f "/root/.ssh/known_hosts" -R 10.208.128.3

# Log out of instance
exit

# Delete instance
nova delete LabKeypairInstance

# List instances, notice status of instance
nova list

# From the dashboard, try booting an instance using the keypair


####################################################################################
####################################################################################
#				Advanced Networking
####################################################################################
####################################################################################


# Use the 'user' credentials
source ~/credentials/user

# Create a total of four instances (exhaust 10.208.128.0/29 subnet IP space)
nova boot --image cirros-qcow2 --flavor m1.micro LabInstance1 --poll
nova boot --image cirros-qcow2 --flavor m1.micro LabInstance2 --poll
nova boot --image cirros-qcow2 --flavor m1.micro LabInstance3 --poll
nova boot --image cirros-qcow2 --flavor m1.micro LabInstance4 --poll

# Create an additional subnet
PRIVATE_SUBNET2_ID=`neutron subnet-create --name private-subnet2 $PRIVATE_NET_ID 10.208.128.8/29 | awk '/ id / { print $4 }'`

# Examine details of new subnet
neutron subnet-show $PRIVATE_SUBNET2_ID

# Create a router and connect the two subnets
ROUTER_ID=`neutron router-create LabRouter | awk '/ id / { print $4 }'`
neutron router-interface-add $ROUTER_ID $PRIVATE_SUBNET1_ID
neutron router-interface-add $ROUTER_ID $PRIVATE_SUBNET2_ID

# Boot a fifth instance
nova boot --image cirros-qcow2 --flavor m1.micro LabInstance5

# Check if any security group rules were created
nova secgroup-list-rules default

# Log into instance in first subnet ( username is 'cirros', password is 'cubswin:)' )
sudo ip netns exec $NETNS_ID ssh cirros@10.208.128.3

# If you get a 'REMOTE HOST IDENTIFICATION HAS CHANGED' warning from previous command
sudo ssh-keygen -f "/root/.ssh/known_hosts" -R 10.208.128.3

# Ping instance in second subnet (should fail, as security groups are enforced between subnets)
ping -c 3 10.208.128.11

# Log out of instance
exit

# Add rules to default security group allowing ping and ssh between instances in the default security group
nova secgroup-add-group-rule default default icmp -1 -1
nova secgroup-add-group-rule default default tcp 22 22

# Wait for rules to take effect...

# Log into instance in first subnet ( username is 'cirros', password is 'cubswin:)' )
sudo ip netns exec $NETNS_ID ssh cirros@10.208.128.3

# Ping instance in second subnet
ping -c 3 10.208.128.11

# Log out of instance
exit

# Create 'neutron' credentials
cat >> ~/credentials/neutron <<EOF
export OS_USERNAME=neutron
export OS_PASSWORD=neutronpw
export OS_TENANT_NAME=Services
export OS_AUTH_URL=http://$MY_IP:5000/v2.0/
export OS_REGION_NAME=RegionOne
export OS_NO_CACHE=1
EOF

# Use the 'neutron' credentials
source ~/credentials/neutron


